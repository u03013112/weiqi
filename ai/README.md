可以连接core，并使用基础功能
可以正常下棋并获得最终成绩
可以获得之前的所有棋谱，进行监督学习

具体的步骤部分还没想好
大体思路有一些：
1、与一般的增强学习不同，更想用监督学习来做
2、从后往前训练，先训练最后一步，这一步可能可以用穷举来做，即如果此时有一步就能赢的情况下，可以用穷举找到这一步的位置。即最后一步是不采用任何ai算法的。
3、倒数第二步开始采用模型训练，这个模型暂时可以选用一种，如果有必要可以将ai分为开局N手ai1，到N手~2N手的ai2以此类推，N可以暂时用10至20，先尝试用1个模型。
4、奖励分数方面提对方一个子奖励1分，被提一子惩罚1分，胜利奖励10分，失败惩罚10分
5、监督学习预测到每一个可用下一步的得分，然后选择得分最高的一个位置下一步。
6、再给输入信号一些额外的辅助信号，这个部分需要再考虑。

2022-07-08
又参观了阿尔法狗的部分方式
可能部分方式是可以借鉴的
1、输入模型：用8X8的矩阵，黑旗一个，白旗一个，该谁下一个。另外，根据我的感觉，考虑将1,2,3口气的旗单独做出来。
没想明白的是是否要将黑白分开考虑，因为目前这个玩法并不是正经围棋，黑白区别不大。如果不将黑白分开，就只区分己方和对手，黑白无关。
2、卷积层，因为是8X8的棋盘，所以卷积怎么做可能不能按照阿尔法那种标准来。甚至我都想不卷积。
3、蒙特卡洛树方面可以先放一放，先看看策略训练的怎么样，mcts暂时只是锦上添花吧，zero那种放到训练里的方式可能太消耗算力，暂时不考虑。

最后一步，或者叫制胜一步还是遍历来做，这样可以加快模型收敛速度。至少在初期这个要手动。

初步计划确实是不分黑白子，用一个ai来做
暂时不考虑间接收益，step按照当前下子方来做？不对，提子肯定是自己这边操作的时候，被提子一定是对方操作的时候，所以这个next state应该是对方操作完的样子。即每一次step应该是自己落一个子，对方落一个子。
就单步的step应该返回的next_state, reward, done, _ 来看，next_state仍然是返回这个子落下之后的样子。但是并不是每次step都会产生一个history。而是当对方落完子，或者游戏结束才算是一个针对自己的history。